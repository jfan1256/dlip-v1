{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ddf7be-ddd6-49d5-b847-8666f42435a7",
   "metadata": {},
   "source": [
    "#### Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa1a281-e4fe-46df-8809-5403981b0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.system import get_config, get_data\n",
    "from class_eval.dlip_eval import DLIPEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7114bc4e-753d-4b1f-a373-a00682751c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device 0: NVIDIA RTX A4000\n",
      "CUDA Device 1: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "# Get Device and Number of Devices\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    for i in range(num_devices):\n",
    "        print(f\"CUDA Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902cbb91-2358-4123-83ff-4c2e69686371",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5b6052-135c-417f-aa8f-53c1b02a23ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "param = json.load(open(get_config() / 'train_config.json'))\n",
    "epoch_number = '2'\n",
    "dlip_bert_pretrain = param['dlip_bert_pretrain']\n",
    "dlip_bert = param['dlip_bert']\n",
    "dlip_vit = param['dlip_vit']\n",
    "dlip_blip = param['dlip_blip']\n",
    "partial = \"True\"\n",
    "print = \"True\"\n",
    "val_split = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1cf575-9e74-4e64-bf7f-efaf9b2b96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DlipEval\n",
    "dlip_eval = DLIPEval(device=device,\n",
    "                         epoch_number=epoch_number,\n",
    "                         dlip_bert_pretrain=dlip_bert_pretrain,\n",
    "                         dlip_bert=dlip_bert,\n",
    "                         dlip_vit=dlip_vit,\n",
    "                         dlip_blip=dlip_blip,\n",
    "                         partial=partial,\n",
    "                         val_split=val_split,\n",
    "                         print=print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "346be93c-b8f4-4261-a5ae-a0129535f1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Loading data...\n",
      "------------------------------------------------------------\n",
      "Creating dataloader...\n",
      "------------------------------------------------------------\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/embeddings/word_embeddings is tied\n",
      "/embeddings/position_embeddings is tied\n",
      "/embeddings/LayerNorm is tied\n",
      "/encoder/layer/0/crossattention/self/query is tied\n",
      "/encoder/layer/0/crossattention/self/key is tied\n",
      "/encoder/layer/0/crossattention/self/value is tied\n",
      "/encoder/layer/0/crossattention/output/dense is tied\n",
      "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
      "/encoder/layer/0/intermediate/dense is tied\n",
      "/encoder/layer/0/output/dense is tied\n",
      "/encoder/layer/0/output/LayerNorm is tied\n",
      "/encoder/layer/1/crossattention/self/query is tied\n",
      "/encoder/layer/1/crossattention/self/key is tied\n",
      "/encoder/layer/1/crossattention/self/value is tied\n",
      "/encoder/layer/1/crossattention/output/dense is tied\n",
      "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
      "/encoder/layer/1/intermediate/dense is tied\n",
      "/encoder/layer/1/output/dense is tied\n",
      "/encoder/layer/1/output/LayerNorm is tied\n",
      "/encoder/layer/2/crossattention/self/query is tied\n",
      "/encoder/layer/2/crossattention/self/key is tied\n",
      "/encoder/layer/2/crossattention/self/value is tied\n",
      "/encoder/layer/2/crossattention/output/dense is tied\n",
      "/encoder/layer/2/crossattention/output/LayerNorm is tied\n",
      "/encoder/layer/2/intermediate/dense is tied\n",
      "/encoder/layer/2/output/dense is tied\n",
      "/encoder/layer/2/output/LayerNorm is tied\n",
      "/encoder/layer/3/crossattention/self/query is tied\n",
      "/encoder/layer/3/crossattention/self/key is tied\n",
      "/encoder/layer/3/crossattention/self/value is tied\n",
      "/encoder/layer/3/crossattention/output/dense is tied\n",
      "/encoder/layer/3/crossattention/output/LayerNorm is tied\n",
      "/encoder/layer/3/intermediate/dense is tied\n",
      "/encoder/layer/3/output/dense is tied\n",
      "/encoder/layer/3/output/LayerNorm is tied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Generating caption...\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DLIP' object has no attribute 'image_text_proj'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m evaluate, val_losses_itm, val_losses_ita, val_losses_caption, val_losses_hr, val_losses_attn \u001b[38;5;241m=\u001b[39m \u001b[43mdlip_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\jonathan\\distillationresearch\\dlip-replication\\dlip\\class_evaluate\\dlip_eval.py:170\u001b[0m, in \u001b[0;36mDLIPEval.evaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m image_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    169\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 170\u001b[0m dlip_caption \u001b[38;5;241m=\u001b[39m \u001b[43mdlip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m generate_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m image_time\n\u001b[0;32m    172\u001b[0m image_per_ms \u001b[38;5;241m=\u001b[39m generate_time \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[1;32mc:\\jonathan\\distillationresearch\\dlip-replication\\dlip\\class_model\\dlip.py:231\u001b[0m, in \u001b[0;36mDLIP.generate\u001b[1;34m(self, image, sample, num_beams, max_length, min_length, top_p, repetition_penalty)\u001b[0m\n\u001b[0;32m    227\u001b[0m     image_attn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(text_image_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(image\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m# Map image_embeds's embed_size -> [batch_size, num_states, embed_size] -> to be the same size as text_embeds's embed size\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m# Example: image_embeds => [3, 197, 192] --> [3, 197, 128]\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     text_image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_text_proj\u001b[49m(image_embeds)\n\u001b[0;32m    232\u001b[0m     image_attn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(text_image_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(image\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sample:\n",
      "File \u001b[1;32m~\\.conda\\envs\\dlip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DLIP' object has no attribute 'image_text_proj'"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "evaluate, val_losses_itm, val_losses_ita, val_losses_caption, val_losses_hr, val_losses_attn = dlip_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b641310-e9b8-4cb3-a146-1d6705da1be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display\n",
    "evaluate.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c99c77-8756-48e3-95a8-03688d4a6483",
   "metadata": {},
   "source": [
    "#### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ec6de-7743-487b-9dff-27602abf277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "def plot_loss(loss, title):\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    plt.plot(loss, label=title)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{title} per Iteration')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8212f-35de-45d6-825d-8b26a7a69de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "losses = json.load(open(get_data() / 'dlip' / 'losses.json'))\n",
    "plot_loss(losses['train_losses_itm'], 'Loss ITM')\n",
    "plot_loss(losses['train_losses_ita'], 'Loss ITA')\n",
    "plot_loss(losses['train_losses_caption'], 'Loss Caption')\n",
    "plot_loss(losses['train_losses_attn'], 'Loss Attention')\n",
    "plot_loss(losses['train_losses_hr'], 'Loss Hidden Representation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4cb6e-bd5e-4dfb-85f8-59e7b2196a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot Validation Loss\n",
    "losses = json.load(open(get_data() / 'dlip' / 'losses.json'))\n",
    "plot_loss(val_losses_itm, 'Loss ITM')\n",
    "plot_loss(val_losses_ita, 'Loss ITA')\n",
    "plot_loss(val_losses_caption, 'Loss Caption')\n",
    "plot_loss(val_losses_hr, 'Loss Attention')\n",
    "plot_loss(val_losses_attn, 'Loss Hidden Representation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dd560-41f6-4269-add4-7793025d2d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlip",
   "language": "python",
   "name": "dlip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
