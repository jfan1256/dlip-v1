{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cdf4c8c-be6f-49f4-9e45-1e26062157b9",
   "metadata": {},
   "source": [
    "#### Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311893d-ad92-45f1-ae39-076710f4dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import lzma\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, BlipForImageTextRetrieval\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "from utils.system import *\n",
    "from class_data.preprocess import Preprocess\n",
    "from class_data.image_tensor import ImageTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1121e5-023a-44f6-9788-58df893fbe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d16df-41c1-4df0-a41f-4e2f023f91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bdf79-4731-4eca-81e7-baff55d35e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead74c8-8600-45de-b012-431e06d73243",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7ebcc-4cbc-4b6c-9126-e2406b66a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = 'all_data_tokenize_*.parquet.brotli'\n",
    "folder_path = get_data() / 'all' / 'chunks'\n",
    "all_data = Preprocess(folder_path=folder_path, file_pattern=file_pattern)._concat_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d6bd24-d982-4ddb-bc3e-379ac34eebbf",
   "metadata": {},
   "source": [
    "#### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35868af1-10b6-4d91-a7f1-ecfa736bd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "# Tiny Vit Requires Image Size of 224\n",
    "image_size = 224\n",
    "batch_size = 6\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a764923-dfc0-4676-85dc-44bdb989c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageTensor(data=all_data, image_column='image_name', caption_column='caption', transform=transform_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa8af1-306c-49db-82cc-d214a32ccf47",
   "metadata": {},
   "source": [
    "#### Get HR and ATTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006aca4-c4ae-481a-8bde-a5becf4a1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings and return file paths\n",
    "def save_embedding(dir, embedding, idx, prefix):\n",
    "    file_path = os.path.join(dir, f\"{prefix}_{idx}.npy.gz\")\n",
    "    with gzip.open(file_path, 'wb') as f:\n",
    "        np.save(f, embedding)\n",
    "    return f\"{prefix}_{idx}.npy.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da980903-abce-4adb-a3a3-b61465709b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Blip Image Captioning Model\n",
    "processor_caption = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_caption = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float32).to(device)\n",
    "# Load in Blip Image Retrieval Model\n",
    "processor_retrieval = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-large-flickr\")\n",
    "blip_retrieval = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-large-flickr\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427cdae-15d6-418c-b0d1-4092628a05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "total_batches = len(dataloader)\n",
    "all_hr_caption = []\n",
    "all_hr_retrieval = []\n",
    "all_attn_caption = []\n",
    "all_attn_retrieval = []\n",
    "all_indices = []\n",
    "export_num = 0\n",
    "\n",
    "for i, (image, caption, idx) in enumerate(dataloader):\n",
    "    with torch.no_grad():\n",
    "        # Log Progress\n",
    "        print(\"-\"*60)\n",
    "        print(f\"Processing batch: {i+1}/{total_batches}...\")\n",
    "\n",
    "        # Load images to device\n",
    "        image = image.to(device, non_blocking=True)\n",
    "        idx = idx.to(device, non_blocking=True)\n",
    "        \n",
    "        # Create compatible image for parent model\n",
    "        parent_image = ((image - image.min()) * (1 / (image.max() - image.min()) * 255)).cpu().numpy().astype('uint8')\n",
    "    \n",
    "        # Caption\n",
    "        inputs_caption = processor_caption(images=parent_image, text=\"a photography of\", return_tensors=\"pt\").to(device, torch.float16)\n",
    "        outputs_caption = blip_caption.forward(**inputs_caption, output_hidden_states=True, output_attentions=True)\n",
    "        hr_parent_caption = torch.stack(outputs_caption.hidden_states)\n",
    "        attn_parent_caption = outputs_caption.attentions[-1]\n",
    "    \n",
    "        # Retrieval\n",
    "        inputs_retrieval = processor_retrieval(images=parent_image, text=\"a photography of\", return_tensors=\"pt\").to(device, torch.float16)\n",
    "        outputs_retrieval = blip_retrieval.forward(**inputs_retrieval, output_hidden_states=True, output_attentions=True)\n",
    "        hr_parent_retrieval = torch.stack(outputs_retrieval.hidden_states)\n",
    "        attn_parent_retrieval = outputs_retrieval.attentions[-1]\n",
    "        print(hr_parent_retrieval.shape)\n",
    "        print(attn_parent_retrieval.shape)\n",
    "\n",
    "        # Store\n",
    "        for j in range(batch_size):\n",
    "            print(hr_parent_caption[:, j, :, :].shape)\n",
    "            print(attn_parent_caption[j, :, :, :].shape)\n",
    "            all_hr_caption.append(hr_parent_caption[:, j, :, :].to(torch.float16).cpu().numpy())\n",
    "            all_hr_retrieval.append(hr_parent_retrieval[:, j, :, :].to(torch.float16).cpu().numpy())\n",
    "            all_attn_caption.append(attn_parent_caption[j, :, :, :].to(torch.float16).cpu().numpy())\n",
    "            all_attn_retrieval.append(attn_parent_retrieval[j, :, :, :].to(torch.float16).cpu().numpy())\n",
    "            all_indices.append(idx[j].item())\n",
    "\n",
    "        # Export in batches\n",
    "        if i%50==0:\n",
    "            print(\"Exporting...\")\n",
    "            embeddings_dir = get_data() / 'blip' / 'emb'\n",
    "            data = {'idx': [], 'hr_caption_path': [], 'hr_retrieval_path': [], 'attn_caption_path': [], 'attn_retrieval_path': []}\n",
    "            \n",
    "            # Save embeddings and get file path name\n",
    "            for i, idx in enumerate(all_indices):\n",
    "                data['idx'].append(idx)\n",
    "                data['hr_caption_path'].append(save_embedding(embeddings_dir, all_hr_caption[i], idx, 'hr_caption'))\n",
    "                data['hr_retrieval_path'].append(save_embedding(embeddings_dir, all_hr_retrieval[i], idx, 'hr_retrieval'))\n",
    "                data['attn_caption_path'].append(save_embedding(embeddings_dir, all_attn_caption[i], idx, 'attn_caption'))\n",
    "                data['attn_retrieval_path'].append(save_embedding(embeddings_dir, all_attn_retrieval[i], idx, 'attn_retrieval'))\n",
    "\n",
    "            # Sort Index\n",
    "            parent_store = pd.DataFrame(data).set_index('idx').sort_index()\n",
    "            # Export Data\n",
    "            parent_store.to_parquet(get_data() / 'blip' / f'blip_store_{export_num}.parquet.brotli', compression='brotli')\n",
    "\n",
    "            # Reset Data\n",
    "            export_num+=1\n",
    "            all_hr_caption = []\n",
    "            all_hr_retrieval = []\n",
    "            all_attn_caption = []\n",
    "            all_attn_retrieval = []\n",
    "            all_indices = []\n",
    "            break    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9ea2a-2d4d-49a6-8527-9d84402a8e80",
   "metadata": {},
   "source": [
    "#### Load HR and ATTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea976f93-2615-40d0-900d-86a226a6b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(dir, filename):\n",
    "    file_path = os.path.join(dir, filename)\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        embedding = np.load(f)\n",
    "    return embedding\n",
    "\n",
    "# Usage\n",
    "embedding_dir = get_data() / 'blip' / 'emb'\n",
    "filename = 'attn_caption_0.npy.gz'\n",
    "\n",
    "loaded_embedding = load_embedding(embedding_dir, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlip-replication",
   "language": "python",
   "name": "dlip-replication"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
